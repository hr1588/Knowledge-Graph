{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59/59 [06:31<00:00,  6.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "크롤링한 기사 개수: 791\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def generate_date_range(start_date, end_date):\n",
    "    start_date = datetime.strptime(start_date, \"%Y.%m.%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y.%m.%d\")\n",
    "    \n",
    "    date_range = []\n",
    "    while start_date <= end_date:\n",
    "        date_range.append(start_date.strftime(\"%Y.%m.%d\"))\n",
    "        start_date += timedelta(days=1)\n",
    "    \n",
    "    return date_range\n",
    "\n",
    "def make_url(search, date, start_pg, end_pg):\n",
    "    start_dt = date\n",
    "    start_dt1 = date.replace(\".\", \"\")\n",
    "    urls = []\n",
    "    for i in range(start_pg, end_pg + 1):\n",
    "        page = 1 + (i-1) * 10\n",
    "        url = f\"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={search}&sort=0&photo=0&field=0&pd=3&ds={start_dt}&de={start_dt}&cluster_rank=27&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:r,p:from{start_dt1}to{start_dt1},a:all&start={page}\"\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "def get_articles(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    articles = soup.select(\"div.group_news > ul.list_news > li div.news_area > div.news_info > div.info_group > a.info\")\n",
    "    return [article[\"href\"] for article in articles if \"news.naver.com\" in article[\"href\"]]\n",
    "\n",
    "def get_article_data(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/98.0.4758.102\"}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "    title = soup.select_one(\"#content > div > div.end_ct_area > h2\")\n",
    "    if title is None:\n",
    "        title = soup.select_one(\"#ct > div.media_end_head.go_trans > div.media_end_head_title > h2\")\n",
    "\n",
    "    date = soup.select_one(\"div.media_end_head_info_datestamp > div > span\")\n",
    "    if date is None:\n",
    "        date = soup.select_one(\"#content > div > div.article_info > span > em\")\n",
    "\n",
    "    content = soup.select_one(\"#dic_area\")\n",
    "    if content is None:\n",
    "        content = soup.select_one(\"#articleBodyContents\")\n",
    "\n",
    "    title = re.sub('<[^>]*>', '', str(title)).strip()\n",
    "    date = date[\"data-date-time\"] if date is not None else re.sub('<[^>]*>', '', str(date)).strip()\n",
    "    content = re.sub('<[^>]*>', '', str(content)).replace('\\n', ' ').strip()\n",
    "    \n",
    "    return title, date, content\n",
    "\n",
    "search = \"가계 경기\"\n",
    "start_date = input(\"크롤링할 시작 날짜를 입력해주세요. (예: 2022.07.01): \")\n",
    "end_date = input(\"크롤링할 마지막 날짜를 입력해주세요. (예: 2022.07.31): \")\n",
    "\n",
    "date_range = generate_date_range(start_date, end_date)\n",
    "\n",
    "news_data = []\n",
    "for date in tqdm(date_range):\n",
    "    urls = make_url(search, date, 1, 3)\n",
    "    for url in urls:\n",
    "        articles = get_articles(url)\n",
    "        for article in articles:\n",
    "            title, date, content = get_article_data(article)\n",
    "            news_data.append((date, title, article, content))\n",
    "\n",
    "news_df = pd.DataFrame(news_data, columns=['date', 'title', 'link', 'content'])\n",
    "news_df = news_df.drop_duplicates(keep='first', ignore_index=True)\n",
    "print(\"크롤링한 기사 개수:\", len(news_df))\n",
    "\n",
    "start_df = start_date.replace('.', '')\n",
    "end_df = end_date.replace('.', '')\n",
    "news_df.to_csv(f'{search}_{start_df}_{end_df}.csv', encoding='utf-8-sig', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
